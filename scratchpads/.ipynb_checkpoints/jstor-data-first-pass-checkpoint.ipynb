{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to Grips with JSTOR data\n",
    "\n",
    "There is a lot of data in the zip files provided by JSTOR. How to get at it properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import pickle as p\n",
    "from math import inf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build input pipeline for Gensim\n",
    "\n",
    "Gensim allows you to stream data when, as in this case, you might have too much to fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSTORCorpus(object):\n",
    "    # Iterator for streaming articles from JSTOR DfR corpus into Gensim\n",
    "    \n",
    "    def __init__(self, meta_dir, data_dir, corpus_meta=None):\n",
    "        self.meta_dir = meta_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.corpus_meta = corpus_meta\n",
    "        self.doc_types = set()\n",
    "        \n",
    "        # Ingest corpus if no existing corpus provided\n",
    "        if self.corpus_meta is None:\n",
    "            self.extract_jstor_meta(self.meta_dir, self.data_dir)\n",
    "        else:\n",
    "            # Otherwise loop over the corpus and extract doc_type information\n",
    "            self.doc_types = set([doc['type'] for key,doc in self.corpus_meta.items()])\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for key in self.corpus_meta:\n",
    "            with open(key) as file:\n",
    "                yield file.read()\n",
    "    \n",
    "    def extract_jstor_meta(self, meta_dir, data_dir):\n",
    "        \"\"\"Loops over directory of JSTOR metadata files, extracts key info from xml\n",
    "\n",
    "        Arguments:\n",
    "        meta_dir (str): directory where metadata files are held\n",
    "        data_dir (str): directory where data files are held\n",
    "        \"\"\"\n",
    "\n",
    "        self.corpus_meta = {}\n",
    "        \n",
    "        parsed = 0\n",
    "        skipped = 0\n",
    "\n",
    "        print(f'Parsing xml files in {meta_dir}. Associated .txt in {data_dir}')\n",
    "        \n",
    "        # The metadata file contains many documents without a text file. We don't want that!\n",
    "        actual_docs = set(os.listdir(data_dir))\n",
    "\n",
    "        for name in tqdm(os.listdir(meta_dir)):\n",
    "            \n",
    "            # Infer name of data file and check\n",
    "            txt_file = name[:-3] + 'txt' # replace .xml with .txt\n",
    "            if txt_file not in actual_docs:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Get doi (for book metadata)\n",
    "            doi = re.sub('^.+_', '', name[:-4])\n",
    "\n",
    "            # Locate data file\n",
    "            data_file = os.path.join(data_dir, txt_file) # fill path\n",
    "            \n",
    "            # Read in metadata file\n",
    "            with open(os.path.join(meta_dir, name)) as file:\n",
    "                meta_xml = BeautifulSoup(file.read())\n",
    "\n",
    "            # Get key metadata\n",
    "            doc_dict = {}\n",
    "\n",
    "            # For articles:\n",
    "            if name.startswith('journal-article'):\n",
    "                doc_dict['type'] = meta_xml.html.body.article['article-type']\n",
    "                # Store doc type in corpus metadata\n",
    "                self.doc_types.add(doc_dict['type'])\n",
    "                title = meta_xml.find(['article-title','source'])\n",
    "                if title is not None:\n",
    "                    doc_dict['title'] = title.get_text()\n",
    "                year = meta_xml.find('year')\n",
    "                if year is not None:\n",
    "                    doc_dict['year'] = year.get_text()\n",
    "\n",
    "            # For book chapters:\n",
    "            elif name.startswith('book-chapter'):\n",
    "                doc_dict['type'] = 'book-chapter'\n",
    "                self.doc_types.add('book-chapter')\n",
    "                # First book-id element is id of whole book\n",
    "                part_of = meta_xml.find('book-id')\n",
    "                if part_of is not None:\n",
    "                    doc_dict['part-of'] = part_of.get_text()\n",
    "                year = meta_xml.find('year')\n",
    "                if year is not None:\n",
    "                    doc_dict['year'] = year.get_text()\n",
    "                # Getting chapter title is slightly harder, because sometimes each book-part is labelled\n",
    "                # simply with the internal id, and sometimes with the doi\n",
    "                book_id = re.sub('.+_', '', doi)\n",
    "                book_rgx = re.compile(re.escape(book_id))\n",
    "                doc_dict['title'] = meta_xml.find('book-part-id', string=book_rgx).parent.find('title').get_text()\n",
    "\n",
    "            # Store in corpus_meta dict\n",
    "            self.corpus_meta[data_file] = doc_dict\n",
    "            \n",
    "            # Increment counter\n",
    "            parsed += 1\n",
    "\n",
    "        # Success message\n",
    "        print(f'{parsed} documents parsed successfully. {skipped} documents skipped.')\n",
    "        \n",
    "    def filter_corpus_by_year(self, min_year=1750, max_year=inf):\n",
    "        \"\"\"Filters the corpus according to minimum and maximum years\n",
    "        \n",
    "        Arguments:\n",
    "        min_year (int)\n",
    "        max_year (int)\"\"\"\n",
    "        \n",
    "        filtered_corpus = {}\n",
    "        \n",
    "        for key,val_dict in self.corpus_meta.items():\n",
    "            # Skip files that cannot be parsed\n",
    "            if 'year' not in val_dict:\n",
    "                continue\n",
    "            try:\n",
    "                year = int(val_dict['year'])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            # Apply conditions\n",
    "            if year <= max_year & year >= min_year:\n",
    "                filtered_corpus[key] = val_dict\n",
    "        \n",
    "        self.meta_corpus = filtered_corpus\n",
    "        \n",
    "    def filter_corpus_by_type(self, allowed_types):\n",
    "        \"\"\"Filters the corpus by doctype.\n",
    "        \n",
    "        Arguments:\n",
    "        allowed_types (list): a list of strings with the allowed doc_types\"\"\"\n",
    "        \n",
    "        filtered_corpus = {}\n",
    "        \n",
    "        for key, val_dict in self.corpus_meta.items():\n",
    "            if val_dict.type in allowed_types:\n",
    "                filtered_corpus[key] = val_dict\n",
    "                \n",
    "        self.meta_corpus = filtered_corpus\n",
    "        \n",
    "    def save(self, path=None):\n",
    "        \"\"\"Pickles the corpus metadata for later use.\n",
    "        \n",
    "        Arguments:\n",
    "        path (str): path to the save file\"\"\"\n",
    "        \n",
    "        if path is None:\n",
    "            path = time.strftime(\"%Y%m%d-%H%M%S\") + '-jstor-corpus.p'\n",
    "        \n",
    "        out = {'meta_dir':self.meta_dir, 'data_dir':self.data_dir, 'corpus_meta':self.corpus_meta}\n",
    "        \n",
    "        with open(path, 'wb') as file:\n",
    "            p.dump(out, file)\n",
    "        \n",
    "        print(f'Corpus saved to {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jstor_corpus(path):\n",
    "    \"\"\"Helper function that loads a pickled corpus created by JSTORCorpus.save()\n",
    "    \n",
    "    Arguments:\n",
    "    path (str): path to the corpus\"\"\"\n",
    "    \n",
    "    with open(path, 'rb') as corpus_file:\n",
    "        corpus = JSTORCorpus(**p.load(corpus_file))\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dir = '../data/metadata'\n",
    "data_dir = '../data/ocr'\n",
    "corpus = JSTORCorpus(meta_dir=meta_dir, data_dir=data_dir)\n",
    "corpus.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
