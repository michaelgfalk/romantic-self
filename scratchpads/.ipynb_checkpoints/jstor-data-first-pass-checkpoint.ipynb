{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to Grips with JSTOR data\n",
    "\n",
    "There is a lot of data in the zip files provided by JSTOR. How to get at it properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle as p\n",
    "from math import inf\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build input pipeline for Gensim\n",
    "\n",
    "Gensim allows you to stream data when, as in this case, you might have too much to fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSTORCorpus(object):\n",
    "    # Iterator for streaming articles from JSTOR DfR corpus into Gensim\n",
    "    \n",
    "    TAG_RGX = re.compile('<[^>]+>') # For cleaning .txt files\n",
    "    \n",
    "    def __init__(self, meta_dir, data_dir, corpus_meta=None):\n",
    "        self.meta_dir = meta_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.corpus_meta = corpus_meta\n",
    "        self.doc_types = set()\n",
    "        \n",
    "        # Ingest corpus if no existing corpus provided\n",
    "        if self.corpus_meta is None:\n",
    "            self.extract_jstor_meta(self.meta_dir, self.data_dir)\n",
    "        else:\n",
    "            # Otherwise loop over the corpus and extract doc_type information\n",
    "            self.doc_types = set([doc['type'] for key,doc in self.corpus_meta.items()])\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for key in self.corpus_meta:\n",
    "            with open(key) as file:\n",
    "                # Get text\n",
    "                raw_xml = file.read()\n",
    "                # Strip tags\n",
    "                text = self.TAG_RGX.sub('', raw_xml)\n",
    "                # Yield array of tokens\n",
    "                yield wordpunct_tokenize(text)\n",
    "    \n",
    "    def extract_jstor_meta(self, meta_dir, data_dir):\n",
    "        \"\"\"Loops over directory of JSTOR metadata files, extracts key info from xml\n",
    "\n",
    "        Arguments:\n",
    "        meta_dir (str): directory where metadata files are held\n",
    "        data_dir (str): directory where data files are held\n",
    "        \"\"\"\n",
    "\n",
    "        self.corpus_meta = {}\n",
    "        \n",
    "        parsed = 0\n",
    "        skipped = 0\n",
    "\n",
    "        print(f'Parsing xml files in {meta_dir}. Associated .txt in {data_dir}')\n",
    "        \n",
    "        # The metadata file contains many documents without a text file. We don't want that!\n",
    "        actual_docs = set(os.listdir(data_dir))\n",
    "\n",
    "        for name in tqdm(os.listdir(meta_dir)):\n",
    "            \n",
    "            # Infer name of data file and check\n",
    "            txt_file = name[:-3] + 'txt' # replace .xml with .txt\n",
    "            if txt_file not in actual_docs:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Get doi (for book metadata)\n",
    "            doi = re.sub('^.+_', '', name[:-4])\n",
    "\n",
    "            # Locate data file\n",
    "            data_file = os.path.join(data_dir, txt_file) # fill path\n",
    "            \n",
    "            # Read in metadata file\n",
    "            with open(os.path.join(meta_dir, name)) as file:\n",
    "                meta_xml = BeautifulSoup(file.read())\n",
    "\n",
    "            # Get key metadata\n",
    "            doc_dict = {}\n",
    "\n",
    "            # For articles:\n",
    "            if name.startswith('journal-article'):\n",
    "                doc_dict['type'] = meta_xml.html.body.article['article-type']\n",
    "                # Store doc type in corpus metadata\n",
    "                self.doc_types.add(doc_dict['type'])\n",
    "                title = meta_xml.find(['article-title','source'])\n",
    "                if title is not None:\n",
    "                    doc_dict['title'] = title.get_text()\n",
    "                year = meta_xml.find('year')\n",
    "                if year is not None:\n",
    "                    doc_dict['year'] = year.get_text()\n",
    "\n",
    "            # For book chapters:\n",
    "            elif name.startswith('book-chapter'):\n",
    "                doc_dict['type'] = 'book-chapter'\n",
    "                self.doc_types.add('book-chapter')\n",
    "                # First book-id element is id of whole book\n",
    "                part_of = meta_xml.find('book-id')\n",
    "                if part_of is not None:\n",
    "                    doc_dict['part-of'] = part_of.get_text()\n",
    "                year = meta_xml.find('year')\n",
    "                if year is not None:\n",
    "                    doc_dict['year'] = year.get_text()\n",
    "                # Getting chapter title is slightly harder, because sometimes each book-part is labelled\n",
    "                # simply with the internal id, and sometimes with the doi\n",
    "                book_id = re.sub('.+_', '', doi)\n",
    "                book_rgx = re.compile(re.escape(book_id))\n",
    "                doc_dict['title'] = meta_xml.find('book-part-id', string=book_rgx).parent.find('title').get_text()\n",
    "\n",
    "            # Store in corpus_meta dict\n",
    "            self.corpus_meta[data_file] = doc_dict\n",
    "            \n",
    "            # Increment counter\n",
    "            parsed += 1\n",
    "\n",
    "        # Success message\n",
    "        print(f'{parsed} documents parsed successfully. {skipped} documents skipped.')\n",
    "        \n",
    "    def filter_by_year(self, min_year=1750, max_year=inf):\n",
    "        \"\"\"Filters the corpus according to minimum and maximum years\n",
    "        \n",
    "        Arguments:\n",
    "        min_year (int)\n",
    "        max_year (int)\"\"\"\n",
    "        \n",
    "        filtered_corpus = {}\n",
    "        \n",
    "        orig_len = len(self.corpus_meta)\n",
    "        print(f'Filtering {orig_len} documents between years {min_year} and {max_year}...')\n",
    "        \n",
    "        for key,val_dict in self.corpus_meta.items():\n",
    "            # Skip files that cannot be parsed\n",
    "            if 'year' not in val_dict:\n",
    "                continue\n",
    "            try:\n",
    "                year = int(val_dict['year'])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            # Apply conditions\n",
    "            if year <= max_year and year >= min_year:\n",
    "                filtered_corpus[key] = val_dict\n",
    "        \n",
    "        self.corpus_meta = filtered_corpus\n",
    "        \n",
    "        print(f'Corpus filtered. {orig_len - len(self.corpus_meta)} documents removed.')\n",
    "        \n",
    "    def filter_by_type(self, allowed_types):\n",
    "        \"\"\"Filters the corpus by doctype.\n",
    "        \n",
    "        Arguments:\n",
    "        allowed_types (list): a list of strings with the allowed doc_types\"\"\"\n",
    "        \n",
    "        filtered_corpus = {}\n",
    "        \n",
    "        orig_len = len(self.corpus_meta)\n",
    "        print(f'Filtering {orig_len} documents ...')\n",
    "        \n",
    "        for key, val_dict in self.corpus_meta.items():\n",
    "            if val_dict.type in allowed_types:\n",
    "                filtered_corpus[key] = val_dict\n",
    "                \n",
    "        self.corpus_meta = filtered_corpus\n",
    "        \n",
    "        print(f'Corpus filtered. {orig_len - len(self.corpus_meta)} documents removed.')\n",
    "        \n",
    "    def save(self, path=None):\n",
    "        \"\"\"Pickles the corpus metadata for later use.\n",
    "        \n",
    "        Arguments:\n",
    "        path (str): path to the save file\"\"\"\n",
    "        \n",
    "        if path is None:\n",
    "            path = time.strftime(\"%Y%m%d-%H%M%S\") + '-jstor-corpus.p'\n",
    "        \n",
    "        out = {'meta_dir':self.meta_dir, 'data_dir':self.data_dir, 'corpus_meta':self.corpus_meta}\n",
    "        \n",
    "        with open(path, 'wb') as file:\n",
    "            p.dump(out, file)\n",
    "        \n",
    "        print(f'Corpus saved to {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jstor_corpus(path):\n",
    "    \"\"\"Helper function that loads a pickled corpus created by JSTORCorpus.save()\n",
    "    \n",
    "    Arguments:\n",
    "    path (str): path to the corpus\"\"\"\n",
    "    \n",
    "    with open(path, 'rb') as corpus_file:\n",
    "        corpus = JSTORCorpus(**p.load(corpus_file))\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_dir = '../data/metadata'\n",
    "# data_dir = '../data/ocr'\n",
    "# corpus = JSTORCorpus(meta_dir=meta_dir, data_dir=data_dir)\n",
    "# corpus.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_jstor_corpus('20200406-052750-jstor-corpus.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test input pipeline with Gensim\n",
    "\n",
    "Gensim provides several methods for modelling a corpus as a whole. Let's try FastText and word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtration: 2284030\n",
      "After filtration: 100000\n"
     ]
    }
   ],
   "source": [
    "print(f'Before filtration: {len(dictionary)}')\n",
    "dictionary.filter_extremes(no_below=2)\n",
    "print(f'After filtration: {len(dictionary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 12700 documents between years 2000 and 2015...\n",
      "Corpus filtered. 398 documents removed.\n"
     ]
    }
   ],
   "source": [
    "# Try on filtered corpus\n",
    "corpus.filter_by_year(min_year=2000, max_year=2015)\n",
    "dictionary = corpora.Dictionary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old len = 1,454,107. New len = 576,987\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=2, keep_n=None)\n",
    "print(f'Old len = 1,454,107. New len = {len(dictionary):,}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
