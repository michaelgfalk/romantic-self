{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to Grips with JSTOR data\n",
    "\n",
    "There is a lot of data in the zip files provided by JSTOR. How to get at it properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import pickle as p\n",
    "from math import inf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser('~') + '/jstor-unzipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data_dir + '/ocr')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at a random article\n",
    "test_article = random.choice(os.listdir(data_dir + '/ocr')).replace('.txt', '')\n",
    "\n",
    "with open(os.path.join(data_dir, 'ocr', test_article + '.txt')) as file:\n",
    "    test_ocr = file.read()\n",
    "\n",
    "with open(os.path.join(data_dir, 'metadata', test_article + '.xml')) as file:\n",
    "    test_xml = BeautifulSoup(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking in the metadata, the most reliable id appears to be the doi. The doi is included in each file name, and appears to also be recorded in the metadata xml.\n",
    "\n",
    "The structure of the metadata also depends on the document type, which is also recorded in the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_rgx = re.compile(r'(?<=-)\\d+.+(?=\\.txt|\\.xml)')\n",
    "type_rgx = re.compile(r'^\\D+(?=-\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if that works on our test example\n",
    "test_id_art = id_rgx.search(os.path.join(data_dir, 'ocr', test_article + '.txt')).group(0)\n",
    "test_id_meta = id_rgx.search(os.path.join(data_dir, 'metadata', test_article + '.xml')).group(0)\n",
    "if test_id_art == test_id_meta:\n",
    "    print('Success!')\n",
    "else:\n",
    "    print('Too bad.')\n",
    "print(f'test_id_art = {test_id_art}')\n",
    "print(f'test_id_meta = {test_id_meta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the type regex\n",
    "all_types = {}\n",
    "errors = []\n",
    "for name in os.listdir(data_dir + '/metadata'):\n",
    "    try:\n",
    "        doc_type = type_rgx.search(name).group(0)\n",
    "        if doc_type in all_types:\n",
    "            all_types[doc_type] += 1\n",
    "        else:\n",
    "            all_types[doc_type] = 1\n",
    "    except AttributeError:\n",
    "        errors.append(name)\n",
    "\n",
    "print(f'There are {len(all_types)} types of document in the corpus:')\n",
    "for dt,n in all_types.items():\n",
    "    print('   - ', dt, ': ', n, sep = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we extract the metadata, let's see how many files there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of OCR files: {len(os.listdir(data_dir + \"/ocr\"))}')\n",
    "print(f'Number of meta files: {len(os.listdir(data_dir + \"/metadata\"))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weirdly it seems there are 10,000 metadata files with no corresponding text file. It seems that the full text is only available for about 400 of the books.\n",
    "\n",
    "Let's do the easy part first: extract all the article_ids from the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doi_from_txt(directory):\n",
    "    \n",
    "    id_rgx = re.compile(r'(?<=-)\\d+.+(?=\\.txt|\\.xml)')\n",
    "    type_rgx = re.compile(r'^\\D+(?=-\\d)')\n",
    "    \n",
    "    # Loops over directory of JSTOR txt files, extracts doi from filename\n",
    "    ocr_ids = {}\n",
    "    for name in os.listdir(directory):\n",
    "        doi = id_rgx.search(name).group(0)\n",
    "        ocr_ids[doi] = name\n",
    "    return ocr_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract the metadata from the xml files.\n",
    "\n",
    "For the book-chapters, we will want to capture data about which book they're from, and whether it is a solo monograph or a collection. I am inclined to count a 'collection' as many texts, and a solo monograph as a single text.\n",
    "\n",
    "Actually, now that I think about it, do I really need to count the corpus? What's the use of that information...?\n",
    "\n",
    "It seems that altough each book-chapter is treated as a seperate object in the data, the metadata file is the metadata for the entire book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at a random book to work out how to get metadata\n",
    "test_book = random.choice([name for name in os.listdir(data_dir + '/metadata') if name.startswith('book')])\n",
    "\n",
    "with open(data_dir + '/metadata/' + test_book) as xml_file:\n",
    "    test_soup = BeautifulSoup(xml_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocr_ids = extract_doi_from_txt(data_dir + '/ocr')\n",
    "# corpus_meta, error = extract_jstor_meta(data_dir + '/metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load the metadata\n",
    "\n",
    "# with open('corpus-meta_20200329.p', 'wb') as file:\n",
    "#     p.dump(corpus_meta, file)\n",
    "\n",
    "with open('corpus-meta_20200329.p', 'rb') as file:\n",
    "    corpus_meta = p.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of each type?\n",
    "all_types = {}\n",
    "no_titles = 0\n",
    "no_years = 0\n",
    "for _,doc_dict in corpus_meta.items():\n",
    "    if doc_dict['type'] not in all_types:\n",
    "        all_types[doc_dict['type']] = 1\n",
    "    else:\n",
    "        all_types[doc_dict['type']] += 1\n",
    "    if 'title' not in doc_dict:\n",
    "        no_titles += 1\n",
    "    if 'year' not in doc_dict:\n",
    "        no_years += 1\n",
    "\n",
    "print(f'There are {len(all_types)} kinds of document in the corpus:')\n",
    "for doc_type,n in all_types.items():\n",
    "    print(f'   - {doc_type}: {n}')\n",
    "print(f'Of the {len(corpus_meta)} documents in the corpus, {no_titles} lack titles, and {no_years} lack dates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_data = {}\n",
    "for _,doc_dict in corpus_meta.items():\n",
    "    if 'year' in doc_dict:\n",
    "        year_int = int(doc_dict['year'])\n",
    "        if year_int in year_data:\n",
    "            year_data[year_int] += 1\n",
    "        else:\n",
    "            year_data[year_int] = 1\n",
    "\n",
    "year_df = pd.DataFrame(list(year_data.items()), columns=['year','n'])\n",
    "year_df.sort_values('year',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df[(year_df.year > 1945) & (year_df.year <= 2015)].plot(x='year', y='n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build input pipeline for Gensim\n",
    "\n",
    "Gensim allows you to stream data when, as in this case, you might have too much to fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSTORCorpus:\n",
    "    # Iterator for streaming articles from JSTOR DfR corpus into Gensim\n",
    "    \n",
    "    def __init__(self, meta_dir, data_dir, corpus_meta=None):\n",
    "        self.meta_dir = meta_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.corpus_meta = corpus_meta\n",
    "        \n",
    "        if self.corpus_meta is None:\n",
    "            self.extract_jstor_meta(self.meta_dir, self.data_dir)\n",
    "        \n",
    "    def __iter__(self)\n",
    "        for key in self.corpus_meta:\n",
    "            with open(key) as file:\n",
    "                yield file.load()\n",
    "    \n",
    "    def extract_jstor_meta(self, meta_dir, data_dir):\n",
    "        \"\"\"Loops over directory of JSTOR metadata files, extracts key info from xml\n",
    "\n",
    "        Arguments:\n",
    "        meta_dir (str): directory where metadata files are held\n",
    "        data_dir (str): directory where data files are held\n",
    "        \"\"\"\n",
    "\n",
    "        self.corpus_meta = {}\n",
    "        \n",
    "        parsed = 0\n",
    "        skipped = 0\n",
    "\n",
    "        print(f'Parsing xml files in {meta_dir}. Associated .txt in {data_dir}')\n",
    "        \n",
    "        # The metadata file contains many documents without a text file. We don't want that!\n",
    "        actual_docs = set(os.listdir(data_dir))\n",
    "\n",
    "        for name in tqdm(os.listdir(meta_dir)):\n",
    "            \n",
    "            # Infer name of data file and check\n",
    "            txt_file = name[:-3] + 'txt' # replace .xml with .txt\n",
    "            if txt_file not in actual_docs:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Locate data file\n",
    "            data_file = os.pathjoin(data_dir, txt_file) # fill path\n",
    "            \n",
    "            # Read in metadata file\n",
    "            with open(os.path.join(meta_dir, name)) as file:\n",
    "                meta_xml = BeautifulSoup(file.read())\n",
    "\n",
    "            # Get key metadata\n",
    "            doc_dict = {}\n",
    "\n",
    "            # For articles:\n",
    "            if name.startswith('journal-article'):\n",
    "                doc_dict['type'] = meta_xml.html.body.article['article-type']\n",
    "                title = meta_xml.find(['article-title','source'])\n",
    "                if title is not None:\n",
    "                    doc_dict['title'] = title.get_text()\n",
    "                year = meta_xml.find('year')\n",
    "                if year is not None:\n",
    "                    doc_dict['year'] = year.get_text()\n",
    "\n",
    "            # For book chapters:\n",
    "            elif name.startswith('book-chapter'):\n",
    "                doc_dict['type'] = 'book-chapter'\n",
    "                # First book-id element is id of whole book\n",
    "                part_of = meta_xml.find('book-id')\n",
    "                if part_of is not None:\n",
    "                    doc_dict['part-of'] = part_of.get_text()\n",
    "                year = meta_xml.find('year')\n",
    "                if year is not None:\n",
    "                    doc_dict['year'] = year.get_text()\n",
    "                # Getting chapter title is slightly harder, because sometimes each book-part is labelled\n",
    "                # simply with the internal id, and sometimes with the doi\n",
    "                book_id = re.sub('.+_', '', doi)\n",
    "                book_rgx = re.compile(re.escape(book_id))\n",
    "                doc_dict['title'] = meta_xml.find('book-part-id', string=book_rgx).parent.find('title').get_text()\n",
    "\n",
    "            # Store in corpus_meta dict\n",
    "            self.corpus_meta[data_file] = doc_dict\n",
    "            \n",
    "            # Increment counter\n",
    "            parsed += 1\n",
    "\n",
    "        # Success message\n",
    "        print(f'{parsed} documents parsed successfully. {skipped} documents skipped.')\n",
    "        \n",
    "    def filter_corpus_by_year(self, min_year=1750, max_year=inf):\n",
    "        \"\"\"Filters the corpus according to minimum and maximum years\n",
    "        \n",
    "        Arguments:\n",
    "        min_year (int)\n",
    "        max_year (int)\"\"\"\n",
    "        \n",
    "        filtered_corpus = {}\n",
    "        \n",
    "        for key,val_dict in self.corpus_meta.items():\n",
    "            # Skip files that cannot be parsed\n",
    "            if 'year' not in val_dict:\n",
    "                continue\n",
    "            try:\n",
    "                year = int(val_dict['year'])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            # Apply conditions\n",
    "            if year <= max_year & year >= min_year:\n",
    "                filtered_corpus[key] = val_dict\n",
    "        \n",
    "        self.meta_corpus = filtered_corpus\n",
    "        \n",
    "    def filter_corpus_by_type(self, allowed_types):\n",
    "        \"\"\"Filters the corpus by doctype.\n",
    "        \n",
    "        Arguments:\n",
    "        allowed_types (list): a list of strings with the allowed doc_types\"\"\"\n",
    "        \n",
    "        filtered_corpus = {}\n",
    "        \n",
    "        for key, val_dict in self.corpus_meta.items():\n",
    "            if val_dict.type in allowed_types:\n",
    "                filtered_corpus[key] = val_dict\n",
    "                \n",
    "        self.meta_corpus = filtered_corpus\n",
    "        \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
